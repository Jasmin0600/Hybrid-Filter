{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0c52fe-f75f-43d9-9673-0a88b93db129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing mp3 files:   0%|                               | 0/4 [00:00<?, ?it/s]/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "processing mp3 files:  25%|█████▌                | 1/4 [03:51<11:33, 231.09s/it]/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "processing mp3 files:  50%|███████████           | 2/4 [08:26<08:33, 256.90s/it]/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "processing mp3 files:  75%|████████████████▌     | 3/4 [12:55<04:22, 262.73s/it]/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:113: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/patel8m6/Desktop/python scripts/mldl/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "processing mp3 files: 100%|██████████████████████| 4/4 [25:10<00:00, 377.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                      transcription\n",
      "0   1   The story of two toys. There seems to be no s...\n",
      "1   2   I'm glad you decided to buy this place. I'm s...\n",
      "2   3   All right. Pat. Crankier. My door was as ugly...\n",
      "3   4   The one man I love is marrying God a kid. Are...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the path to your FFmpeg executable\n",
    "os.environ[\"WHISPER_FFMPEG\"] = \"C:/ffmpeg/bin/ffmpeg.exe\"\n",
    "\n",
    "# Use CPU as the device\n",
    "device = \"cpu\"\n",
    "\n",
    "# models to use\n",
    "\"\"\"\n",
    "tiny    39 M   tiny.en    tiny   ~1 GB  ~32x\n",
    "base    74 M   base.en    base   ~1 GB  ~16x\n",
    "small   244 M  small.en   small  ~2 GB  ~6x\n",
    "medium  769 M  medium.en  medium ~5 GB  ~2x\n",
    "\"\"\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = whisper.load_model(\"small.en\", device=device)\n",
    "\n",
    "# Directory containing the .mp3 files\n",
    "audio_dir = \"audio test\"\n",
    "\n",
    "# List to store the transcription data\n",
    "transcriptions = []\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for file_name in tqdm(os.listdir(audio_dir), desc=\"processing mp3 files\"):\n",
    "    if file_name.endswith('.mp3'):\n",
    "        # Extract the id from the file name (e.g., \"1.mp3\" -> 1)\n",
    "        file_id = int(os.path.splitext(file_name)[0])\n",
    "        \n",
    "        # Path to the audio file\n",
    "        file_path = os.path.join(audio_dir, file_name)\n",
    "        \n",
    "        # Transcribe the audio file\n",
    "        result = model.transcribe(file_path)\n",
    "        \n",
    "        # Get the transcription text\n",
    "        transcription_text = result['text']\n",
    "        \n",
    "        # Append the transcription to the list\n",
    "        transcriptions.append({\"id\": file_id, \"transcription\": transcription_text})\n",
    "\n",
    "# Create a DataFrame from the transcriptions list\n",
    "df = pd.DataFrame(transcriptions, columns=[\"id\", \"transcription\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"transcriptions.csv\", index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cc87a-f43b-49d9-b91f-db4406f02847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
